# Introduction to Algorithmic Information Theory, Belief Propagation, Graph Signal Processing, and Deep Reinforcement Learning
---

## Information Information Theory

Compression (Entropy):

<img src="https://latex.codecogs.com/gif.latex?H(X)&space;=&space;-\sum_{x}&space;p(x)\log_{2}p(x)" title="H(X) = -\sum_{x} p(x)\log_{2}p(x)" />

Capacity:

<img src="https://latex.codecogs.com/gif.latex?\begin{align*}&space;SIR&space;&=&space;I(X{;}Y)&space;&=&space;H(X)&space;-&space;H(X|Y)&space;\end{align*}" title="\begin{align*} SIR &= I(X{;}Y) &= H(X) - H(X|Y) \end{align*}" />

## Bayesian Statistics
Belief Propagation:
We can use Belief propagation to calculate the channel Capacity of some Markov chain...[and so on](m_data_comp.md)
